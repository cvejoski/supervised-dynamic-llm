{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff9906-c7ba-4327-a2b0-b2b04f61f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import matplotlib.pylab as plt\n",
    "import cv2\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from kiwissenbase.data import dataloaders\n",
    "from kiwissenbase.data.datasets import CaltechPedestrian, CityPersons\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a38e9-ce82-41b4-aeec-348955777abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "invTrans = torchvision.transforms.Compose([ torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ]),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e8d52-2cc2-491d-97cf-0e295312f38b",
   "metadata": {},
   "source": [
    "## Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf710e8f-91f9-42b3-9606-b4f697ab0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_args = {\"root_dir\": \"/data/anna/data/caltech_dataset/\",\n",
    "                    \"batch_size\": 8,\n",
    "                    \"validation_batch_size\": 4,\n",
    "                    \"num_workers\": 0,\n",
    "                    \"pin_memory\": True,\n",
    "                    #\"collate_fn\": collate_fn,\n",
    "                    \"normal_mean\": (0.5, 0.5, 0.5),\n",
    "                    \"normal_std\": (0.5, 0.5, 0.5),\n",
    "                    \"different_size_target\": True,\n",
    "                    \"subset\": \"annotated-pedestrians\",                    \n",
    "                    \"target_transform\":{\"module\": \"kiwissenbase.models.object_detection\",\n",
    "                                        \"class_name\": \"FasterRCNN\",\n",
    "                                        \"method_name\": \"target_transform\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d13aae-4a08-4d5f-8034-eba1b9eb5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = dataloaders.CaltechPedastrianDataLoader(device=\"cpu\", **data_loader_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de454d2e-9be2-4ff2-8255-86d4193e7a42",
   "metadata": {},
   "source": [
    "## Functions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792689a-584d-479e-9d73-ecf2623edbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bbs(target_boxes, predicted_boxes, iou_thres=0):\n",
    "    \n",
    "    matched_inds = []\n",
    "    \n",
    "    if len(target_boxes) > 0 and len(predicted_boxes) > 0:\n",
    "        target_boxes = torch.stack(target_boxes)\n",
    "        predicted_boxes = torch.stack(predicted_boxes)\n",
    "    \n",
    "        pairwise_iou = torchvision.ops.box_iou(target_boxes,predicted_boxes)\n",
    "        # only consider cases with an iou more than the given threshold\n",
    "        pairwise_iou[pairwise_iou<iou_thres] = 0\n",
    "\n",
    "        while len((pairwise_iou > 0).nonzero()):\n",
    "            max_iou = torch.max(pairwise_iou)\n",
    "            max_inds = (pairwise_iou==max_iou).nonzero()\n",
    "            matched_inds.append({\"target_index\":max_inds[0][0].item(), \"predicted_index\":max_inds[0][1].item(), \"iou\":max_iou.item()})\n",
    "            pairwise_iou[max_inds[0][0],:] = 0\n",
    "            pairwise_iou[:,max_inds[0][1]] = 0\n",
    "\n",
    "    all_target_inds_matched = [item[\"target_index\"] for item in matched_inds]\n",
    "    for ind in range(len(target_boxes)):\n",
    "        if ind not in all_target_inds_matched:\n",
    "            matched_inds.append({\"target_index\":ind, \"predicted_index\":-1, \"iou\": 0})\n",
    "    all_predicted_inds_matched = [item[\"predicted_index\"] for item in matched_inds]\n",
    "    for ind in range(len(predicted_boxes)):\n",
    "        if ind not in all_predicted_inds_matched:\n",
    "            matched_inds.append({\"target_index\":-1, \"predicted_index\":ind, \"iou\": 0})\n",
    "    return matched_inds\n",
    "    \n",
    "    \n",
    "def evaluate_prediction(target, prediction, score_thres=0, iou_thres=0):\n",
    "    \n",
    "    pedestrian_target_boxes = [box for box,label in zip(target[\"boxes\"],target[\"labels\"]) if label==1]\n",
    "    # fitler the predictions based on a minimum score threshold\n",
    "    pedestrian_output_boxes = [box for box,label,score in zip(prediction[\"boxes\"],prediction[\"labels\"],prediction[\"scores\"])\n",
    "                               if label==1 and score>=score_thres]\n",
    "    \n",
    "    matched_boxes = match_bbs(pedestrian_target_boxes, pedestrian_output_boxes, iou_thres)\n",
    "\n",
    "    tps = 0\n",
    "    fps = 0\n",
    "    fns = 0\n",
    "    for match in matched_boxes:\n",
    "        if match[\"target_index\"] == -1:\n",
    "            fps += 1\n",
    "        elif match[\"predicted_index\"] == -1:\n",
    "            fns += 1\n",
    "        else:\n",
    "            tps += 1\n",
    "    if tps+fps > 0:\n",
    "        precision = tps/(tps +fps)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if tps + fns > 0:\n",
    "        recall = tps/(tps+fns)\n",
    "    else:\n",
    "        recall = 0\n",
    "    return {\"tps\": tps, \"fps\": fps, \"fns\": fns, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6dd1c9-5cb7-4aca-87fd-db5bc131c259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4764bc40-bf6a-48d6-ac5a-01c89c18d95f",
   "metadata": {},
   "source": [
    "## Select paths for trained model and netdissect resuts (run one of the following blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d506c8-298e-4bff-b210-b38e61c97c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_saved_model = \"/data/anna/pedestrian_detection_models/faster_rcnn_tuned_Caltech.pth\"\n",
    "\n",
    "path_to_result = \"/data/anna/results/pytorch_fasterrcnn_resnet50_fpn_caltech_backbone.body.layer4[1].conv2/\"\n",
    "layer_dissected = \"backbone.body.layer4[1].conv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bf49f5-b176-4fc4-800c-68855a5012e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_saved_model = \"/data/anna/pedestrian_detection_models/faster_rcnn_tuned_Citypersons_all_single_person_classes_positive.pth\"\n",
    "path_to_result = \"/data/anna/results/pytorch_fasterrcnn_resnet50_fpn_citypersons/\"\n",
    "layer_dissected = \"backbone.body.layer4[1].conv2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aef2d43-d2c3-4616-9724-70ce0c28a2a2",
   "metadata": {},
   "source": [
    "## Select dataset on which to check the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8fa77-49e9-4677-ae0f-df9ad84d2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"caltech\" # caltech or citypersons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40304165-2363-4802-830b-b86e31ee010c",
   "metadata": {},
   "source": [
    "## setup model, netdissect results and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f93f1e4-f847-424e-b4de-825d07c48611",
   "metadata": {},
   "source": [
    "### load and test saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff6291c-71df-4db4-b25b-6f35c6c0fad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(path_to_saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1a346-385e-476e-8593-caa8649b85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model architecture and adapt output\n",
    "import torchvision \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn()\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb6a4da-f2b1-4e89-a1e3-ee1832ed9817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a42d2a-c058-4306-b174-c616b53e26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9553abf1-990a-4172-a207-a3333220e18b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7026a0b5-78e5-4015-8aeb-fd590eb64d6d",
   "metadata": {},
   "source": [
    "### register forward hooks on the dissected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596c956-2823-417e-bd6b-9cacb2c942d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        unit_activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0818bf-b073-48e8-9c59-77286c1c6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_activations = {}\n",
    "conv_unit = layer_dissected\n",
    "eval(f\"model.{conv_unit}.register_forward_hook\")(get_activation(conv_unit))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee5c6c-f469-4bc8-a5eb-6f8e41aac360",
   "metadata": {},
   "source": [
    "### test model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2f736-0576-4703-af7e-f8b2944d8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "image,targets = next(iter(loader.test.dataset))\n",
    "image.to(device)\n",
    "predictions = model([image.to(device)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78383ff-5b80-4d8e-99e2-f77d8d35089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08432adb-3ddc-4eb7-a0f0-39189fc7b485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unit_activations[layer_dissected].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b4389e-39b4-45e8-a785-88142aca910f",
   "metadata": {},
   "source": [
    "### load relevant netdissect results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc4c0b5-e6d0-48c3-a6d8-e234912765e5",
   "metadata": {},
   "source": [
    "#### tally.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068f195-f8ad-4f4a-ae47-9c2cd91d453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tally = os.path.join(path_to_result,\"tally.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde4056a-8080-4ac8-9d4b-e54425f4656b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = [line for line in csv.reader(open(path_to_tally))]\n",
    "data_all = [{\"unit\": int(item[0])-1, \"concept\":item[2], \"score\":item[3]} for item in data[1:]]\n",
    "data_top = data_all[:30]\n",
    "print(\"Top scoring units\\n\")\n",
    "pd.DataFrame(data_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4efcdf1-78ce-4c41-892b-bcc6ad33ebb5",
   "metadata": {},
   "source": [
    "#### filter the relevant concept units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce834b7b-e96f-4f69-b5bb-06736537a06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"all concepts in layer\")\n",
    "print(set([item[2] for item in data[1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b32eaaa-06f7-441a-aeef-728bf74422a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_concepts = [\"head\", \"hair\", \"arm\", \"wheel\", \"car\", \"sidewalk\",\"road\",\"neck\",\"mouth\",\"person\",\"leg\",\"back\",\"foot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4f83ca-9b5d-4a76-a741-f1d0f25e7718",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_top_relevant = [{\"unit\": int(item[0])-1, \"concept\":item[2], \"score\":item[3]} for item in data[1:] if item[2] in relevant_concepts][:30]\n",
    "print(\"Top scoring relevant units\\n\")\n",
    "pd.DataFrame(data_top_relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def735ac-e2ba-4cca-a6fc-98420ddb790f",
   "metadata": {},
   "source": [
    "#### quantile.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aea107a-29b7-4764-9561-31e7723a802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = np.load(os.path.join(path_to_result,\"quantile.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96384fb1-a172-427c-b66e-0e9d9c511374",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantile.shape)\n",
    "print(quantile[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7828caf3-14e0-46cb-8a2a-1a0109b181a4",
   "metadata": {},
   "source": [
    "## Collect the targets, predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44715ec-8f6d-40b1-b110-f103c9dccbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pedestrian_image = []\n",
    "detected_pedestrian_image = []\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for local_batch in tqdm(loader.test):\n",
    "        pedestrian_image.extend([any(item[\"labels\"]==1) for item in local_batch[1]])\n",
    "        local_batch_images = torch.stack(local_batch[0]).to(device)\n",
    "        all_targets.extend(local_batch[1])\n",
    "        output = model(local_batch_images)\n",
    "        output_cpu = [{\"boxes\": item[\"boxes\"].to(\"cpu\"), \"labels\": item[\"labels\"].to(\"cpu\"), \"scores\": item['scores'].to(\"cpu\")}for item in output]\n",
    "        all_predictions.extend(output_cpu)\n",
    "        detected_pedestrian_image.extend([any(item[\"labels\"]==1) for item in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0727d-74ea-416c-a5e4-0bd97595756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total images\", len(pedestrian_image))\n",
    "print(\"images with at least one labelled pedestrian\", sum(pedestrian_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2062c-0c28-4315-8596-8f50cc24deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"images with at least one detected pedestrian(no score threshold)\", sum(detected_pedestrian_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85290e-8837-4332-89ad-834f69f00213",
   "metadata": {},
   "source": [
    "### check performance on different score/iou thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f73e0-1918-4f05-8bb3-070eb0589c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_thresholds = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "iou_thresholds = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "for score_thres in score_thresholds:\n",
    "    for iou_thres in iou_thresholds:\n",
    "        avg_precision = 0\n",
    "        avg_recall = 0\n",
    "        for target, prediction in zip(all_targets,all_predictions):\n",
    "            res = evaluate_prediction(target,prediction,score_thres=score_thres, iou_thres=iou_thres)\n",
    "            avg_precision += res[\"precision\"]\n",
    "            avg_recall += res[\"recall\"]\n",
    "        avg_precision = avg_precision/len(all_targets)\n",
    "        avg_recall = avg_recall/len(all_targets)\n",
    "        print(f\"score threshold {score_thres}, iou threshold {iou_thres}\")\n",
    "        print(f\"average precision {avg_precision}, average recall {avg_recall}\")\n",
    "        print(\"-------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e56f0f-42ff-4b48-aa0d-18d6f17c8025",
   "metadata": {},
   "source": [
    "### select thresholds and collect the performance for all images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f69bca-7237-4a8f-b8b1-eacfd729a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_thres=0.6\n",
    "iou_thres=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0623263-acdc-48ab-bc6e-30b1d7470ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_precision = []\n",
    "all_recall = []\n",
    "for target, prediction in zip(all_targets,all_predictions):\n",
    "    res = evaluate_prediction(target,prediction,score_thres=score_thres, iou_thres=iou_thres)\n",
    "    all_precision.append(res[\"precision\"])\n",
    "    all_recall.append(res[\"recall\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9158f-ff66-4527-a6c7-fa0a48bd7852",
   "metadata": {},
   "source": [
    "## Project activation back to image space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d024a8c-5fa6-4417-b825-e6ef585c0f74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcbd79f-6517-4dff-b158-d11183e1b452",
   "metadata": {},
   "source": [
    "### select image index and convolutional unit index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f7aac-6e5c-4cd9-8fb1-921ef5cb722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ind = 16\n",
    "unit_ind = 427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1078dfa-0e7d-4999-957a-9e3ac5a24912",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,target = loader.test.dataset.__getitem__(img_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f8d79-366f-4d90-baf7-b663f23393be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need inverse transform to display image\n",
    "img_display = invTrans(img.cpu()).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90763b9f-b291-490d-9d60-448c3c3f96b5",
   "metadata": {},
   "source": [
    "### get ouput and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf120d3-c389-4808-a3b6-5b6521a821b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.stack([img]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea1a95-1abd-4232-a8df-672938dfb981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df04d07-a2c0-4491-91e4-cde3c81db482",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_prediction(target,output[0],score_thres=0.6, iou_thres=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5218adc0-4a83-4664-8cc2-c7df6be1ccc7",
   "metadata": {},
   "source": [
    "### get activation, upsample to image dim, create mask based on quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fb28c-3ec8-47e4-93a1-af69ed39e678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activation = unit_activations[conv_unit][0]\n",
    "activation_resized = torchvision.transforms.functional.resize(activation,img.shape[1:])\n",
    "mask = activation_resized[unit_ind]>quantile[unit_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7da6e-ed17-4054-902e-39231d46b482",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6c087-3587-4da1-966e-34f515c56ab7",
   "metadata": {},
   "source": [
    "#### show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7878f99e-8c1c-4573-8a4e-d9c0741cab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(img_display, (1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc0833-6060-42a2-917f-f5d8c8d609f7",
   "metadata": {},
   "source": [
    "#### show image with detections/labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed15381d-fcfb-4970-b95a-525d2aa00e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_boxes = [box for box,score in zip(output[0][\"boxes\"],output[0][\"scores\"]) if score>score_thres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70939890-d89d-42d1-9ae7-fd09d907bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cv2 = img_display.transpose(1,2,0)\n",
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_RGB2BGR)\n",
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adea221b-4904-4e0d-9978-dc331b8746c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axs = plt.subplots(ncols= 1, squeeze=False, figsize=(20, 10))\n",
    "\n",
    "for box_ind,box in enumerate(target[\"boxes\"]):\n",
    "    box = [int(i.item()) for i in box]\n",
    "    cv2.rectangle(img_cv2, (box[0],box[1], box[2]-box[0],box[3]-box[1]), color=(0, 255, 0),thickness=2)\n",
    "    cv2.putText(img_cv2,f\"t_{box_ind}\", (box[0],box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0),thickness=2)\n",
    "for box_ind,box in enumerate(output_boxes):\n",
    "    box = [int(i.item()) for i in box]\n",
    "    cv2.rectangle(img_cv2, (box[0],box[1], box[2]-box[0],box[3]-box[1]), color=(255, 0, 0),thickness=2)\n",
    "    cv2.putText(img_cv2,f\"p_{box_ind}\", (box[0],box[1]-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0),thickness=2)\n",
    "axs[0, 0].imshow(img_cv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd36cf-f502-4f78-a720-3e387d6f07fb",
   "metadata": {},
   "source": [
    "#### show activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba03989-83f8-4857-8dde-071f56f8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axs = plt.subplots(ncols= 2,nrows=2, squeeze=False, figsize=(20, 10))\n",
    "axs[0, 0].imshow(np.transpose(img_display, (1,2,0)))\n",
    "axs[0, 0].set_title(\"image\")\n",
    "axs[0, 1].imshow(activation[unit_ind].cpu().numpy(), cmap='gray')\n",
    "axs[0, 1].set_title(\"raw activation\")\n",
    "axs[1, 0].imshow(activation_resized[unit_ind].cpu().numpy(), cmap='gray')\n",
    "axs[1, 0].set_title(\"activation interpolated to image dimensions\")\n",
    "axs[1, 1].imshow(mask.cpu().numpy())\n",
    "axs[1, 1].set_title(\"masked activation (significant)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94be2f1d-1c31-4b04-b454-30b2baaa515d",
   "metadata": {},
   "source": [
    "#### show image, overlayed with masked activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6687e7-a915-4cd1-8129-5548d3afc243",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axs = plt.subplots(ncols= 1, squeeze=False, figsize=(15, 15))\n",
    "axs[0, 0].imshow(np.transpose(img_display, (1,2,0)))\n",
    "#plt.imshow(act_resized[510].cpu().numpy(), cmap='gray', alpha=0.6)\n",
    "axs[0, 0].imshow(mask.cpu().numpy(), cmap='gray', alpha=0.4)\n",
    "axs[0,0].set_title(f\"image with activation of unit {unit_ind} ({[item['concept'] for item in data_top_relevant if item['unit']==unit_ind][0]})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de93a85-2fcf-42e5-9dc7-1f2d1acffe0d",
   "metadata": {},
   "source": [
    "### Plot multiple units/detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555762dc-9a3e-4f26-873f-f73d557d9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = data_top_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7892d-5077-4d3d-89e1-33caaa1bdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 4\n",
    "nrows = math.ceil(len(detectors)/ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2966d0e-93a4-48ec-95f0-10b8175890ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axs = plt.subplots(ncols= ncols, nrows = nrows, squeeze=False, figsize=(30, 55))\n",
    "ind = 0\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        if ind>=len(detectors):\n",
    "            break\n",
    "        mask = activation_resized[detectors[ind][\"unit\"]]>quantile[detectors[ind][\"unit\"]]\n",
    "        axs[i, j].imshow(np.transpose(img_display, (1,2,0)))\n",
    "        axs[i, j].imshow(mask.cpu().numpy(), cmap='gray', alpha=0.4)\n",
    "        axs[i, j].set_title(f\"{detectors[ind]['unit']}_{detectors[ind]['concept']}\")\n",
    "        ind +=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cc5ae-dbae-43c0-9fe2-ed599bcdd320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d692a-fa38-432a-9c4d-100535a18df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
